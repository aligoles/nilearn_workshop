{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nDifferent classifiers in decoding the Haxby dataset\n=====================================================\n\nHere we compare different classifiers on a visual object recognition\ndecoding task.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We start by loading the data and applying simple transformations to it\n-----------------------------------------------------------------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Fetch data using nilearn dataset fetcher\nfrom nilearn import datasets\n# by default 2nd subject data will be fetched\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint('First subject anatomical nifti image (3D) located is at: %s' %\n      haxby_dataset.anat[0])\nprint('First subject functional nifti image (4D) is located at: %s' %\n      haxby_dataset.func[0])\n\n# load labels\nimport numpy as np\nlabels = np.recfromcsv(haxby_dataset.session_target[0], delimiter=\" \")\nstimuli = labels['labels']\n# identify resting state labels in order to be able to remove them\nresting_state = stimuli == b'rest'\n\n# find names of remaining active labels\ncategories = np.unique(stimuli[np.logical_not(resting_state)])\n\n# extract tags indicating to which acquisition run a tag belongs\nsession_labels = labels[\"chunks\"][np.logical_not(resting_state)]\n\n# Load the fMRI data\nfrom nilearn.input_data import NiftiMasker\n\n# For decoding, standardizing is often very important\nmask_filename = haxby_dataset.mask_vt[0]\nmasker = NiftiMasker(mask_img=mask_filename, standardize=True)\nfunc_filename = haxby_dataset.func[0]\nmasked_timecourses = masker.fit_transform(\n    func_filename)[np.logical_not(resting_state)]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Then we define the various classifiers that we use\n---------------------------------------------------\nA support vector classifier\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.svm import SVC\nsvm = SVC(C=1., kernel=\"linear\")\n\n# The logistic regression\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, \\\n    RidgeClassifierCV\nlogistic = LogisticRegression(C=1., penalty=\"l1\")\nlogistic_50 = LogisticRegression(C=50., penalty=\"l1\")\nlogistic_l2 = LogisticRegression(C=1., penalty=\"l2\")\n\n# Cross-validated versions of these classifiers\nfrom sklearn.grid_search import GridSearchCV\n# GridSearchCV is slow, but note that it takes an 'n_jobs' parameter that\n# can significantly speed up the fitting process on computers with\n# multiple cores\nsvm_cv = GridSearchCV(SVC(C=1., kernel=\"linear\"),\n                      param_grid={'C': [.1, .5, 1., 5., 10., 50., 100.]},\n                      scoring='f1', n_jobs=1)\n\nlogistic_cv = GridSearchCV(LogisticRegression(C=1., penalty=\"l1\"),\n                           param_grid={'C': [.1, .5, 1., 5., 10., 50., 100.]},\n                           scoring='f1')\nlogistic_l2_cv = GridSearchCV(LogisticRegression(C=1., penalty=\"l2\"),\n                              param_grid={\n                                  'C': [.1, .5, 1., 5., 10., 50., 100.]},\n                              scoring='f1')\n\n# The ridge classifier has a specific 'CV' object that can set it's\n# parameters faster than using a GridSearchCV\nridge = RidgeClassifier()\nridge_cv = RidgeClassifierCV()\n\n# A dictionary, to hold all our classifiers\nclassifiers = {'SVC': svm,\n               'SVC cv': svm_cv,\n               'log l1': logistic,\n               'log l1 50': logistic_50,\n               'log l1 cv': logistic_cv,\n               'log l2': logistic_l2,\n               'log l2 cv': logistic_l2_cv,\n               'ridge': ridge,\n               'ridge cv': ridge_cv}"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Here we compute prediction scores\n----------------------------------\nRun time for all these classifiers\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Make a data splitting object for cross validation\nfrom sklearn.cross_validation import LeaveOneLabelOut, cross_val_score\ncv = LeaveOneLabelOut(session_labels)\n\nimport time\n\nclassifiers_scores = {}\n\nfor classifier_name, classifier in sorted(classifiers.items()):\n    classifiers_scores[classifier_name] = {}\n    print(70 * '_')\n\n    for category in categories:\n        task_mask = np.logical_not(resting_state)\n        classification_target = (stimuli[task_mask] == category)\n        t0 = time.time()\n        classifiers_scores[classifier_name][category] = cross_val_score(\n            classifier,\n            masked_timecourses,\n            classification_target,\n            cv=cv, scoring=\"f1\")\n\n        print(\"%10s: %14s -- scores: %1.2f +- %1.2f, time %.2fs\" % (\n            classifier_name, category,\n            classifiers_scores[classifier_name][category].mean(),\n            classifiers_scores[classifier_name][category].std(),\n            time.time() - t0))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Then we make a rudimentary diagram\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import matplotlib.pyplot as plt\nplt.figure()\n\ntick_position = np.arange(len(categories))\nplt.xticks(tick_position, categories, rotation=45)\n\nfor color, classifier_name in zip(\n        ['b', 'c', 'm', 'g', 'y', 'k', '.5', 'r', '#ffaaaa'],\n        sorted(classifiers)):\n    score_means = [classifiers_scores[classifier_name][category].mean()\n                   for category in categories]\n    plt.bar(tick_position, score_means, label=classifier_name,\n            width=.11, color=color)\n    tick_position = tick_position + .09\n\nplt.ylabel('Classification accurancy (f1 score)')\nplt.xlabel('Visual stimuli category')\nplt.ylim(ymin=0)\nplt.legend(loc='lower center', ncol=3)\nplt.title('Category-specific classification accuracy for different classifiers')\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Finally, w plot the face vs house map for the different classifiers\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Use the average EPI as a background\nfrom nilearn import image\nmean_epi_img = image.mean_img(func_filename)\n\n# Restrict the decoding to face vs house\ncondition_mask = np.logical_or(stimuli == b'face', stimuli == b'house')\nmasked_timecourses = masked_timecourses[\n    condition_mask[np.logical_not(resting_state)]]\nstimuli = stimuli[condition_mask]\n# Transform the stimuli to binary values\nstimuli = (stimuli == b'face').astype(np.int)\n\nfrom nilearn.plotting import plot_stat_map, show\n\nfor classifier_name, classifier in sorted(classifiers.items()):\n    classifier.fit(masked_timecourses, stimuli)\n\n    if hasattr(classifier, 'coef_'):\n        weights = classifier.coef_[0]\n    elif hasattr(classifier, 'best_estimator_'):\n        weights = classifier.best_estimator_.coef_[0]\n    else:\n        continue\n    weight_img = masker.inverse_transform(weights)\n    weight_map = weight_img.get_data()\n    threshold = np.max(np.abs(weight_map)) * 1e-3\n    plot_stat_map(weight_img, bg_img=mean_epi_img,\n                  display_mode='z', cut_coords=[-15],\n                  threshold=threshold,\n                  title='%s: face vs house' % classifier_name)\n\nshow()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.13", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}